Il **Pipelining** è una tecnica che migliora l'efficienza nell'esecuzione delle istruzioni, aumentando il numero totale di istruzioni eseguite in un periodo di tempo determinato dal *ciclo di clock*
### Data Path e Stages
I 5 stage nell'esecuzione di un istruzione possono essere rappresentati:
- :
	- ![[Pasted image 20240128172145.png]]
Implementando la tecnica del Pipelining ciascuna istruzione richiede (almeno) un solo ciclo di clock.
*Funzionamento*:
- mentre lo stage $k$ sta eseguendo l'istruzione $I_j$, lo stage precedente $k - 1$ è libero
- lo stage $k - 1$ può pertanto eseguire l'istruzione $I_{j + 1}$
### Data Dependency Hazard
E' sempre possibile il Pipelining? No.
Questo perché esistono casi in cui le istruzioni $I_j$ e $I_{j + 1}$ hanno una **data dependency** tra loro.
Cosa fare:
- occorre *bloccare* (mettere in **stallo**) la pipeline
- $I_{j+1}$ deve attendere il completamento fino allo stage 5 dell'istruzione $I_j$
- la condizione che provoca uno stallo della pipeline viene detta **hazard**
*esempio*:
- `ADD R2, R3, #100` e `SUB R9, R2 #30`: 
	- ![[Pasted image 20240128173259.png]]
		 l'operazione di `SUB` deve attendere il completamento della `ADD`
#### Soluzione: Operand Forwarding
Il Data Path viene modificato in modo da far sì che l'*output dello stage 3* possa essere (anche) inviato all'input dell'ALU:
- ![[Pasted image 20240128173739.png]]
#### Soluzione: NOP Insertion
Le NOP permettono alla `ADD` di completare il percorso nel Data Path:
- ![[Pasted image 20240128173916.png]]
### Memory Delay Hazard
L'accesso alla memoria richiede (in genere) vari cicli di clock. Se la dipendenza include una `LOAD` o `STORE`, occorre *bloccare la pipeline* per tutta la durata dell'accesso alla memoria.
*esempio*:
- `LDR R2, [R3]` e `SUB R9, R2 #30`:
	- ![[Pasted image 20240128174235.png]]
#### Soluzione: Operand Forwarding
L'*uscita dello stage 4 viene inviata come input* aggiuntivo dello stage 3 della prossima istruzione:
- ![[Pasted image 20240128174352.png]]
### Branch Hazard
L'istruzione di **branch** implica una "penalità" di efficienza. 
#### Branch Incondizionati
Il branch con destinazione **assoluta** comporta un ritardo di esecuzione già a partire dalla *fase di fetch*:
- ![[Pasted image 20240128174630.png]]
Il branch con destinazione **relativa** comporta un ritardo di esecuzione a partire dalla *fase di compute*:
- ![[Pasted image 20240128174811.png]]
#### Branch Condizionati
Il branch condizionato implica la *valutazione di una condizione*.
In funzione della condizione occorre eseguire il fetch o dell'**istruzione successiva** o dell'**istruzione all'indirizzo target**: prevale $I_{j+1}$ o $I_k$?
##### Static Branch Prediction
Si decide **a priori** (design-time) se effettuare il fetch di $I_{j+1}$ o $I_t$ e ogni volta che la *predizione è errata* si paga la **branch penalty**
##### Dynamic Branch Prediction
Si utilizza una macchina a stati finiti con *due* stati:
- **LT** = Branch *L*ikely to be *T*aken ($I_t$)
- **LNT** = Branch *L*ikely *N*ot to be *T*aken ($I_{j+1}$)
Si decide lo stato iniziale e ogni volta che la *predizione è errata* si paga la **branch penalty**
- :
	- ![[Pasted image 20240128175617.png]]
##### More Accurate Dynamic Branch Prediction
Si utilizza una macchina a stati finiti con *quattro* stati:
- **ST** = Strongly likely to be Taken (salto molto probabilmente effettuato $I_t$)
- **LT** = Branch *L*ikely to be *T*aken (salto probabilmente effettuato $I_t$)
- **LNT** = Branch *L*ikely *N*ot to be *T*aken ($I_{j+1}$)
- **SNT** = Strongly likely Not to be Taken (salto molto probabilmente non effettuato $I_{j+1}$)
- :
	- ![[Pasted image 20240128180409.png]]
